# 예제 3.53~3.56 이진 분류 코드 전체 설명

---

## 전체 프로세스 플로우차트

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         1. 데이터 준비 (Data Preparation)                     │
├─────────────────────────────────────────────────────────────────────────────┤
│  binary.csv 로드 → [x1, x2, x3] 특성 + [y] 레이블 (0 또는 1)                  │
│                              ↓                                               │
│  train(80%) / validation(10%) / test(10%) 분할                              │
│                              ↓                                               │
│  DataLoader로 배치(batch) 단위로 묶기 (batch_size=64)                         │
└─────────────────────────────────────────────────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                         2. 모델 정의 (Model Definition)                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│   입력 [x1, x2, x3]  ──→  Linear(3→1)  ──→  Sigmoid  ──→  출력 (0~1)        │
│        (3개 특성)         (가중치 연산)      (활성화함수)     (확률값)          │
│                                                                              │
│   계산: output = sigmoid(w1*x1 + w2*x2 + w3*x3 + b)                         │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                         3. 학습 루프 (Training Loop) × 10,000 epochs          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  A. 순전파 (Forward Pass)                                             │   │
│  │     입력 x → 모델 → 예측값 output (0~1 사이 확률)                      │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                               ↓                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  B. 손실 계산 (Loss Calculation)                                      │   │
│  │     BCELoss(output, y) → 예측이 정답과 얼마나 다른지 계산              │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                               ↓                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  C. 역전파 (Backward Pass)                                            │   │
│  │     loss.backward() → 각 가중치가 손실에 미친 영향(기울기) 계산        │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                               ↓                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  D. 가중치 업데이트 (Weight Update)                                   │   │
│  │     optimizer.step() → 기울기 방향으로 가중치 조정 (w = w - lr*grad)   │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  → 이 과정을 반복하면서 손실(cost)이 점점 줄어듦                              │
└─────────────────────────────────────────────────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                         4. 검증 (Validation)                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│   검증 데이터 입력 → 모델 예측 → 0.5 기준으로 분류                            │
│                                                                              │
│   output >= 0.5  →  클래스 1 (True)                                         │
│   output <  0.5  →  클래스 0 (False)                                        │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 블록 1: Import

```python
import torch
import pandas as pd
from torch import nn
from torch import optim
from torch.utils.data import Dataset, DataLoader, random_split
```

### nn (Neural Network)

신경망을 만들기 위한 도구 모음

| 포함된 것 | 예시 | 용도 |
|-----------|------|------|
| **레이어** | `nn.Linear`, `nn.Conv2d` | 신경망 층 정의 |
| **활성화 함수** | `nn.Sigmoid`, `nn.ReLU` | 비선형 변환 |
| **손실 함수** | `nn.BCELoss`, `nn.MSELoss` | 오차 계산 |
| **모델 베이스** | `nn.Module` | 커스텀 모델 상속용 |

### optim (Optimizer)

가중치를 업데이트하는 최적화 알고리즘 모음

| 옵티마이저 | 특징 |
|------------|------|
| `optim.SGD` | 기본 경사하강법 (이 코드에서 사용) |
| `optim.Adam` | 가장 많이 사용, 빠른 수렴 |
| `optim.RMSprop` | 학습률 자동 조절 |

### Dataset, DataLoader, random_split

데이터를 효율적으로 다루기 위한 도구

- **Dataset**: "데이터를 어떻게 읽을지" 정의하는 틀
- **DataLoader**: Dataset에서 데이터를 배치(batch) 단위로 꺼내주는 역할
- **random_split**: 데이터를 랜덤하게 분할

```
전체 데이터: [1,2,3,4,5,6,7,8,9,10]
                    ↓ DataLoader (batch_size=3, shuffle=True)
배치1: [5,2,9]
배치2: [1,7,4]
배치3: [10,3,6]
배치4: [8]
```

### 요약 다이어그램

```
┌─────────────────────────────────────────────────────────┐
│                      PyTorch 구조                        │
├─────────────────────────────────────────────────────────┤
│                                                          │
│   pandas ──→ 데이터 로드 (CSV)                           │
│                  ↓                                       │
│   Dataset ──→ 데이터 정의 (어떻게 읽을지)                 │
│                  ↓                                       │
│   random_split → 데이터 분할 (train/val/test)            │
│                  ↓                                       │
│   DataLoader ─→ 배치 단위로 공급                         │
│                  ↓                                       │
│   nn ────────→ 모델 구조 정의                            │
│                  ↓                                       │
│   optim ─────→ 가중치 업데이트                           │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

---

## 블록 2: CustomDataset 클래스

```python
class CustomDataset(Dataset):
    def __init__(self, file_path):
        df = pd.read_csv(file_path)
        self.x1 = df.iloc[:, 0].values
        self.x2 = df.iloc[:, 1].values
        self.x3 = df.iloc[:, 2].values
        self.y = df.iloc[:, 3].values
        self.length = len(df)

    def __getitem__(self, index):
        x = torch.FloatTensor([self.x1[index], self.x2[index], self.x3[index]])
        y = torch.FloatTensor([int(self.y[index])])
        return x, y

    def __len__(self):
        return self.length
```

### 전체 구조

```
Dataset (PyTorch 제공)
    ↑ 상속
CustomDataset (우리가 만듦)
    │
    ├── __init__()      → 데이터 로드 (1번만 실행)
    ├── __getitem__()   → 1개 샘플 반환 (DataLoader가 호출)
    └── __len__()       → 전체 개수 반환
```

### `__init__` (초기화)

CSV 파일 구조:
```
┌────────┬────────┬────────┬───────┐
│   x1   │   x2   │   x3   │   y   │
├────────┼────────┼────────┼───────┤
│  1.2   │  0.7   │  1.1   │   0   │  ← index 0
│  0.3   │  0.2   │  0.4   │   0   │  ← index 1
│  3.2   │  2.7   │  3.1   │   1   │  ← index 2
│  ...   │  ...   │  ...   │  ...  │
└────────┴────────┴────────┴───────┘
     ↑        ↑        ↑        ↑
  iloc[:,0] iloc[:,1] iloc[:,2] iloc[:,3]
```

`iloc` 설명:
```python
df.iloc[:, 0]         # 모든 행(:), 0번째 열
df.iloc[:, 0].values  # numpy 배열로 변환
```

### `__getitem__` (데이터 1개 반환)

동작 예시:
```
__getitem__(0) 호출 시:

    self.x1[0] = 1.2
    self.x2[0] = 0.7    →  x = tensor([1.2, 0.7, 1.1])
    self.x3[0] = 1.1

    self.y[0] = 0       →  y = tensor([0.0])

    return (tensor([1.2, 0.7, 1.1]), tensor([0.0]))
           ─────────────────────────────────────────
                   특성(입력)          레이블(정답)
```

### `__len__` (전체 개수)

DataLoader가 "데이터가 몇 개야?" 물을 때 대답
→ 배치를 몇 번 만들지 계산하는 데 사용

### DataLoader와의 연동

```
┌─────────────────────────────────────────────────────────┐
│  DataLoader(dataset, batch_size=64)                     │
│                                                          │
│  1. len(dataset) 호출 → __len__() → 100개               │
│                                                          │
│  2. 배치 생성 시:                                        │
│     dataset[0] → __getitem__(0) → (x, y)                │
│     dataset[1] → __getitem__(1) → (x, y)                │
│     ...                                                  │
│     dataset[63] → __getitem__(63) → (x, y)              │
│                                                          │
│  3. 64개를 묶어서 하나의 배치로 반환                      │
│     x: tensor([64, 3])  ← 64개 샘플, 각 3개 특성         │
│     y: tensor([64, 1])  ← 64개 샘플, 각 1개 레이블       │
└─────────────────────────────────────────────────────────┘
```

### 메서드 요약

| 메서드 | 호출 시점 | 역할 |
|--------|-----------|------|
| `__init__` | `CustomDataset(파일경로)` | CSV 로드, 데이터 저장 |
| `__getitem__` | `dataset[index]` | index번째 (x, y) 반환 |
| `__len__` | `len(dataset)` | 전체 데이터 개수 반환 |

> CustomDataset = CSV 파일을 PyTorch가 이해할 수 있는 형태로 변환하는 어댑터

---

## 블록 3: CustomModel 클래스

```python
class CustomModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = nn.Sequential(
          nn.Linear(3, 1),
          nn.Sigmoid()
        )

    def forward(self, x):
        x = self.layer(x)
        return x
```

### 전체 구조

```
nn.Module (PyTorch 제공)
    ↑ 상속
CustomModel (우리가 만듦)
    │
    ├── __init__()   → 모델 구조 정의 (레이어 설계)
    └── forward()    → 순전파 정의 (데이터가 어떻게 흐르는지)
```

### `__init__` (모델 구조 정의)

- `super().__init__()`: nn.Module의 기능을 사용하기 위한 필수 코드 (가중치 관리, GPU 이동, 모델 저장/불러오기)
- `nn.Sequential`: 레이어들을 순서대로 연결하는 컨테이너

#### `nn.Linear(3, 1)`

```
┌─────────────────────────────────────┐
│         nn.Linear(3, 1)             │
│                                     │
│   x1 ──→ ×w1 ──┐                   │
│                 │                   │
│   x2 ──→ ×w2 ──┼──→ Σ + b ──→ out │
│                 │                   │
│   x3 ──→ ×w3 ──┘                   │
│                                     │
│   학습되는 파라미터:                 │
│   - w1, w2, w3 (가중치 3개)         │
│   - b (편향 1개)                    │
│   - 총 4개                          │
└─────────────────────────────────────┘
```

#### `nn.Sigmoid()`

출력을 0~1 사이로 압축:
```
           1
σ(x) = ─────────
        1 + e⁻ˣ
```

```
Linear 출력값     Sigmoid 통과 후
───────────────────────────────────
   -10.0      →      0.00004
    -2.0      →      0.12
     0.0      →      0.5
     2.0      →      0.88
    10.0      →      0.99996
```

### `forward` (순전파)

```
입력 x = [1.2, 0.7, 1.1]
              ↓
┌─────────────────────────────────────────────┐
│  nn.Linear(3, 1)                            │
│  1.2×w1 + 0.7×w2 + 1.1×w3 + b = 2.5 (예시) │
└─────────────────────────────────────────────┘
              ↓
         중간값: 2.5
              ↓
┌─────────────────────────────────────────────┐
│  nn.Sigmoid()                               │
│  σ(2.5) = 1/(1+e⁻²·⁵) = 0.924              │
└─────────────────────────────────────────────┘
              ↓
출력: 0.924 (클래스 1일 확률 92.4%)
```

`model(x)` 호출 시 자동으로 `forward(x)` 실행. 직접 `model.forward(x)`는 권장하지 않음.
`model(x)` 호출 시 PyTorch가 hook 실행, gradient 추적 등 추가 작업을 수행하기 때문.

### 모델 요약

| 구성요소 | 역할 | 파라미터 수 |
|----------|------|-------------|
| `nn.Linear(3,1)` | 특성들의 가중합 계산 | 4개 (w1,w2,w3,b) |
| `nn.Sigmoid()` | 확률값으로 변환 | 0개 |
| **총합** | | **4개** |

> 이 모델 = 로지스틱 회귀 (Logistic Regression)

---

## 블록 4: 데이터 분할 및 DataLoader

```python
dataset = CustomDataset("../datasets/binary.csv")
dataset_size = len(dataset)
train_size = int(dataset_size * 0.8)
validation_size = int(dataset_size * 0.1)
test_size = dataset_size - train_size - validation_size

train_dataset, validation_dataset, test_dataset = random_split(
    dataset, [train_size, validation_size, test_size], torch.manual_seed(4)
)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)
validation_dataloader = DataLoader(validation_dataset, batch_size=4, shuffle=True, drop_last=True)
test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, drop_last=True)
```

### 크기 계산

```python
train_size      = int(100 * 0.8)   # 80
validation_size = int(100 * 0.1)   # 10
test_size       = 100 - 80 - 10    # 10
```

test_size만 뺄셈인 이유:
```
int()는 소수점을 버림
예: 데이터 97개일 때
  train = int(97 * 0.8) = int(77.6) = 77
  val   = int(97 * 0.1) = int(9.7)  = 9
  test  = 97 - 77 - 9              = 11  ← 나머지 전부 흡수
```

### 데이터 분할

```
전체 100개 (랜덤으로 섞은 뒤 분할)
┌──────────────────────────────────────────────────────────┐
│  Train (80개)           │ Val (10개)  │ Test (10개)       │
│  모델 학습용             │ 학습 중     │ 최종 성능         │
│                         │ 검증용      │ 평가용            │
└──────────────────────────────────────────────────────────┘
```

| 데이터셋 | 비율 | 용도 |
|----------|------|------|
| **Train** | 80% | 가중치를 학습시키는 데 사용 |
| **Validation** | 10% | 학습 중간에 성능 확인 (과적합 감지) |
| **Test** | 10% | 학습 끝난 후 최종 성능 측정 |

`torch.manual_seed(4)`: 매번 같은 순서로 분할 → 실행할 때마다 동일한 결과 보장

### DataLoader 파라미터

| 파라미터 | 값 | 의미 |
|----------|-----|------|
| `batch_size` | 64 / 4 | 한 번에 꺼내는 샘플 수 |
| `shuffle` | True | 매 epoch마다 순서를 섞음 |
| `drop_last` | True | 마지막 남는 배치 버림 |

`drop_last=True` 이유:
```
Train 80개, batch_size=64 일 때:

drop_last=False          drop_last=True
─────────────           ─────────────
배치1: 64개              배치1: 64개
배치2: 16개 (불균형!)    (16개는 버림)

→ 배치 크기가 다르면 학습이 불안정할 수 있음
```

### 전체 흐름

```
binary.csv
     ↓
CustomDataset (100개 샘플)
     ↓
random_split
     ↓
┌────────────────┬──────────────┬──────────────┐
│  Train (80개)  │  Val (10개)  │ Test (10개)  │
└───────┬────────┴──────┬───────┴──────┬───────┘
        ↓               ↓              ↓
   DataLoader       DataLoader     DataLoader
   batch=64         batch=4        batch=4
        ↓               ↓              ↓
  ┌──────────┐    ┌──────────┐   ┌──────────┐
  │ 배치1:64 │    │ 배치1: 4 │   │ 배치1: 4 │
  │ (나머지  │    │ 배치2: 4 │   │ 배치2: 4 │
  │  16 버림)│    │ (나머지  │   │ (나머지  │
  └──────────┘    │  2 버림) │   │  2 버림) │
                  └──────────┘   └──────────┘
       ↓               ↓              ↓
   학습에 사용      중간 검증      최종 평가
```

---

## 블록 5: 모델·손실함수·옵티마이저 설정

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CustomModel().to(device)
criterion = nn.BCELoss().to(device)
optimizer = optim.SGD(model.parameters(), lr=0.0001)
```

### device 설정

```
NVIDIA GPU가 있는가?
    ├── Yes → "cuda" (GPU 사용)
    └── No  → "cpu"  (CPU 사용)

GPU: 행렬 연산을 병렬로 처리 → 빠름
CPU: 순차적으로 처리           → 느림

모델과 데이터가 같은 장치에 있어야 연산 가능
```

### 모델 생성

```
CustomModel()  → 모델 객체 생성 (가중치 랜덤 초기화)
.to(device)    → 해당 장치로 이동

┌────────────────────────────────┐
│  CustomModel                   │
│                                │
│  w1 = 랜덤  ┐                 │
│  w2 = 랜덤  ├→ 학습 대상       │
│  w3 = 랜덤  │                 │
│  b  = 랜덤  ┘                 │
│                                │
│  장치: cpu (또는 cuda)         │
└────────────────────────────────┘
```

### 손실 함수 (criterion) - BCELoss

BCE = Binary Cross Entropy: "예측값이 정답과 얼마나 다른가"를 숫자로 계산

공식:
```
Loss = -[y × log(output) + (1-y) × log(1-output)]
```

동작 예시:
```
정답 y=1 일 때:
┌────────────────────────────────────┐
│  예측값    계산                손실 │
│  0.9      -log(0.9)  =       0.10 │  ← 잘 맞춤 (낮음)
│  0.5      -log(0.5)  =       0.69 │
│  0.1      -log(0.1)  =       2.30 │  ← 못 맞춤 (높음)
└────────────────────────────────────┘

정답 y=0 일 때:
┌────────────────────────────────────┐
│  예측값    계산                손실 │
│  0.1      -log(0.9)  =       0.10 │  ← 잘 맞춤 (낮음)
│  0.5      -log(0.5)  =       0.69 │
│  0.9      -log(0.1)  =       2.30 │  ← 못 맞춤 (높음)
└────────────────────────────────────┘

핵심: 예측이 틀릴수록 손실이 커짐 → 가중치를 더 크게 조정
```

### 옵티마이저 - SGD

SGD = Stochastic Gradient Descent (확률적 경사 하강법)

| 파라미터 | 값 | 의미 |
|----------|-----|------|
| `model.parameters()` | w1, w2, w3, b | 업데이트할 대상 |
| `lr` | 0.0001 | 학습률 (한 번에 얼마나 조정할지) |

학습률(lr)의 영향:
```
lr이 너무 크면:            lr이 적절하면:         lr이 너무 작으면:
    ╲  ╱                      ↘                      ·
     ╲╱  발산                  ↘                     ·
     ╱╲                         ↘                   ·
    ╱  ╲                         ● 최적점           ·
                                                   · (도달 못함)
```

가중치 업데이트 공식:
```
w_new = w_old - lr × gradient

예시:
  w = 0.5
  gradient = 2.0
  lr = 0.0001

  w_new = 0.5 - 0.0001 × 2.0
        = 0.4998

→ 아주 조금씩 조정됨 (lr이 작으므로)
```

### 이 4줄의 역할 요약

```
┌──────────────────────────────────────────────────────┐
│                    학습 준비 완료                      │
│                                                       │
│  device    → 어디서 계산할지     (CPU/GPU)             │
│  model     → 무엇으로 예측할지   (Linear + Sigmoid)   │
│  criterion → 얼마나 틀렸는지     (BCELoss)            │
│  optimizer → 어떻게 고칠지       (SGD, lr=0.0001)     │
│                                                       │
│         이 4개가 갖춰지면 학습 루프 시작 가능           │
└──────────────────────────────────────────────────────┘
```

---

## 보충: gradient vs weight

```
weight (가중치, w)       = 모델이 학습하는 값 자체
gradient (기울기, ∂L/∂w) = 그 가중치를 어느 방향으로 얼마나 바꿔야 하는지
```

비유:
```
산에서 내려가는 등산객

현재 위치     = weight (가중치)
발 밑 경사면  = gradient (기울기)
한 걸음 크기  = learning rate (학습률)

목표: 가장 낮은 곳(최소 손실)에 도달
```

계산 흐름:
```
1. 현재 w = 0.5         ← weight (가중치)
2. 이 w로 예측 → 손실(Loss) 계산
3. loss.backward() 실행
   → gradient 계산: ∂L/∂w = 2.0    ← "w를 줄여야 손실이 줄어든다"
4. optimizer.step() 실행
   → w_new = w - lr × gradient
   → w_new = 0.5 - 0.0001 × 2.0
   → w_new = 0.4998               ← 업데이트된 weight
```

| 항목 | 정체 | 예시 | 누가 만드는가 |
|------|------|------|----------------|
| **weight** | 모델의 파라미터 | w = 0.5 | 초기: 랜덤, 이후: optimizer가 업데이트 |
| **gradient** | 손실의 변화율 | ∂L/∂w = 2.0 | `loss.backward()`가 계산 |
| **lr** | 보폭 크기 | 0.0001 | 사람이 설정 |

```
weight   → "지금 여기 있다"
gradient → "이 방향으로 가라"
lr       → "이만큼만 가라"
```

---

## 블록 6: 학습 루프

```python
for epoch in range(10000):
    cost = 0.0

    for x, y in train_dataloader:
        x = x.to(device)
        y = y.to(device)

        output = model(x)
        loss = criterion(output, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        cost += loss

    cost = cost / len(train_dataloader)

    if (epoch + 1) % 1000 == 0:
        print(f"Epoch : {epoch+1:4d}, Model : {list(model.parameters())}, Cost : {cost:.3f}")
```

### 전체 구조

```
바깥 반복: epoch (10,000번 반복)
│
└── 안쪽 반복: batch (train_dataloader에서 배치 단위로)
    │
    ├── 순전파      output = model(x)
    ├── 손실 계산    loss = criterion(output, y)
    ├── 기울기 초기화 optimizer.zero_grad()
    ├── 역전파      loss.backward()
    └── 가중치 수정  optimizer.step()
```

### 에폭 반복

```
epoch = 전체 데이터를 1번 다 보는 것

epoch 1: 전체 데이터 한 바퀴
epoch 2: 전체 데이터 한 바퀴
...
epoch 10000: 전체 데이터 한 바퀴

cost = 이번 epoch의 총 손실 (매 epoch마다 0으로 초기화)
```

### 배치 반복

```
train_dataloader가 배치를 하나씩 꺼내줌

배치1: x = [64, 3] 텐서,  y = [64, 1] 텐서
          ↑ 64개 샘플, 3개 특성     ↑ 64개 정답

모델이 cuda에 있으면 데이터도 cuda로 이동해야 연산 가능
```

### 순전파 (Forward)

```
x [64, 3] → Linear(3,1) → Sigmoid → output [64, 1]

예시 (1개 샘플):
  [1.2, 0.7, 1.1] → 1.2×w1 + 0.7×w2 + 1.1×w3 + b → σ(결과) → 0.82
```

### 손실 계산

```
예측값 output과 정답 y를 비교

  output: [0.82, 0.15, 0.91, ...]   (모델이 예측한 확률)
  y:      [1.0,  0.0,  1.0,  ...]   (실제 정답)
                    ↓
  BCELoss 계산 → loss = 0.25 (하나의 숫자)
```

### 핵심 3줄 (학습이 일어나는 부분)

```
① zero_grad() - 이전 기울기 지우기
┌────────────────────────────┐
│ w1.grad = 0                │
│ w2.grad = 0                │
│ w3.grad = 0                │
│ b.grad  = 0                │
└────────────────────────────┘
   왜? PyTorch는 기울기를 누적하므로 매번 초기화 필요

        ↓

② backward() - 기울기 계산
┌────────────────────────────┐
│ w1.grad = ∂Loss/∂w1 = 0.3 │
│ w2.grad = ∂Loss/∂w2 = 0.1 │
│ w3.grad = ∂Loss/∂w3 = 0.5 │
│ b.grad  = ∂Loss/∂b  = 0.2 │
└────────────────────────────┘
   손실을 줄이려면 각 가중치를 어느 방향으로 바꿔야 하는지

        ↓

③ step() - 가중치 업데이트
┌────────────────────────────────────────────┐
│ w1 = w1 - 0.0001 × 0.3 = w1 - 0.00003    │
│ w2 = w2 - 0.0001 × 0.1 = w2 - 0.00001    │
│ w3 = w3 - 0.0001 × 0.5 = w3 - 0.00005    │
│ b  = b  - 0.0001 × 0.2 = b  - 0.00002    │
└────────────────────────────────────────────┘
```

### 손실 누적 및 출력

```
epoch 1:    cost = 0.693  (초반, 손실 높음)
epoch 1000: cost = 0.412
epoch 2000: cost = 0.285
...
epoch 10000: cost = 0.031  (학습 완료, 손실 낮음)

→ cost가 줄어드는 것 = 모델이 학습되고 있다는 증거
```

### 예측값이 나오는 구체적 계산 과정

초기 상태 (학습 전, 가중치는 랜덤):
```
w1 = 0.2,  w2 = -0.3,  w3 = 0.5,  b = 0.1  (랜덤 초기값)
입력: x = [1.2, 0.7, 1.1],  정답 y = 0
```

Step 1 - Linear (가중합):
```
z = w1×x1 + w2×x2 + w3×x3 + b
z = 0.2×1.2 + (-0.3)×0.7 + 0.5×1.1 + 0.1
  = 0.24    + (-0.21)    + 0.55     + 0.1
  = 0.68
```

Step 2 - Sigmoid (확률 변환):
```
output = 1 / (1 + e^(-0.68))
       = 1 / (1 + 0.507)
       = 1 / 1.507
       = 0.664
```

결과: 예측 0.664 (66.4% 확률로 클래스 1) / 정답 0 → 틀림! → 손실 큼 → 가중치 수정됨

학습이 진행되면:
```
epoch 1 (랜덤 가중치):
  입력 [1.2, 0.7, 1.1] → 0.664 (정답 0인데 0.664 → 틀림)
       ↓ 손실 계산 → 역전파 → 가중치 수정

epoch 100:
  입력 [1.2, 0.7, 1.1] → 0.412 (조금 나아짐)
       ↓ 계속 수정...

epoch 5000:
  입력 [1.2, 0.7, 1.1] → 0.083 (거의 0에 가까움 → 맞춤!)
```

가중치가 바뀌면 같은 입력이라도 다른 예측이 나옴:
```
epoch 1:     w1=0.2,  w2=-0.3, w3=0.5,  b=0.1  → 0.664
epoch 5000:  w1=-1.2, w2=-0.8, w3=-1.1, b=0.3  → 0.083
```

### 1 epoch 전체 흐름 요약

```
epoch 시작 (cost = 0)
│
├── 배치1 (64개 샘플)
│   입력 → 예측 → 손실계산 → 기울기초기화 → 역전파 → 가중치수정
│   cost += loss
│
├── (남은 16개는 drop_last=True로 버림)
│
├── cost = cost / 배치수
│
└── 1000번째면 출력

이것을 10,000번 반복
→ 가중치가 점점 최적값에 가까워짐
→ 손실이 점점 줄어듦
→ 예측이 점점 정확해짐
```

---

## 블록 7: 검증

```python
with torch.no_grad():
    model.eval()
    for x, y in validation_dataloader:
        x = x.to(device)
        y = y.to(device)

        outputs = model(x)

        print(outputs)
        print(outputs >= torch.FloatTensor([0.5]).to(device))
        print("--------------------")
```

### `torch.no_grad()`

```
학습 루프                     검증 루프
──────────                    ──────────
gradient 계산 필요 (O)        gradient 계산 불필요 (X)
loss.backward() 하니까        가중치 수정 안 하니까

→ no_grad()로 gradient 계산을 꺼버림
→ 메모리 절약, 속도 향상
```

### `model.eval()`

```
모드 전환:

model.train()  ← 학습 중 (기본값)
model.eval()   ← 평가 중

이 모델에서는 차이가 없지만,
Dropout이나 BatchNorm이 있는 모델에서는 동작이 달라짐:

         train 모드          eval 모드
Dropout  뉴런 랜덤 비활성화   모든 뉴런 사용
BatchNorm 배치 통계 사용      전체 통계 사용
```

### 결과 출력

출력 예시:
```
tensor([[0.0312],        ← 3.1% 확률
        [0.9451],        ← 94.5% 확률
        [0.1208],        ← 12.1% 확률
        [0.8837]])       ← 88.4% 확률

tensor([[False],         ← 0.0312 < 0.5 → 클래스 0
        [ True],         ← 0.9451 ≥ 0.5 → 클래스 1
        [False],         ← 0.1208 < 0.5 → 클래스 0
        [ True]])        ← 0.8837 ≥ 0.5 → 클래스 1
--------------------
```

판정 기준:
```
               0.5 (임계값)
                │
   클래스 0     │     클래스 1
◄──────────────┼──────────────►
0.0           0.5            1.0
                │
  output < 0.5  │  output ≥ 0.5
  → False (0)   │  → True (1)
```

### 학습 vs 검증 비교

```
┌──────────────────────────────────────────────────────────┐
│  학습 루프 (train)                                        │
│                                                           │
│  for x, y in train_dataloader:                           │
│      output = model(x)         ← 예측                    │
│      loss = criterion(output, y) ← 손실 계산              │
│      optimizer.zero_grad()     ← 기울기 초기화            │
│      loss.backward()           ← 역전파                   │
│      optimizer.step()          ← 가중치 수정              │
│                                                           │
│  목적: 가중치를 최적화                                    │
└──────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────┐
│  검증 루프 (validation)                                   │
│                                                           │
│  with torch.no_grad():         ← gradient 끔             │
│      model.eval()              ← 평가 모드                │
│      for x, y in validation_dataloader:                  │
│          outputs = model(x)    ← 예측만                   │
│          print(outputs)        ← 결과 확인                │
│                                                           │
│  목적: 학습된 모델이 새 데이터도 잘 맞추는지 확인          │
└──────────────────────────────────────────────────────────┘
```

### 검증을 하는 이유

```
학습 데이터로만 평가하면?
→ "시험 문제를 미리 알고 시험 본 것"
→ 당연히 점수 높음 (과적합 위험)

검증 데이터로 평가하면?
→ "처음 보는 문제로 시험 본 것"
→ 진짜 실력 확인 가능
```

---

## 코드와 프로세스 매핑 종합

| 프로세스 단계 | 코드 |
|---------------|------|
| 데이터 로드 | `CustomDataset("binary.csv")` |
| 데이터 분할 | `random_split(dataset, [train, val, test])` |
| 배치 생성 | `DataLoader(dataset, batch_size=64)` |
| 모델 정의 | `nn.Linear(3, 1)` + `nn.Sigmoid()` |
| 손실 함수 | `nn.BCELoss()` |
| 옵티마이저 | `optim.SGD(lr=0.0001)` |
| 순전파 | `output = model(x)` |
| 손실 계산 | `loss = criterion(output, y)` |
| 역전파 | `loss.backward()` |
| 가중치 업데이트 | `optimizer.step()` |
| 기울기 초기화 | `optimizer.zero_grad()` |
| 검증 | `model.eval()` + `torch.no_grad()` |
