ML 에 있어서 batch normalization이 무엇인지 쉽게 설명해 보세요

배치 정규화(Batch Normalization)는 신경망 학습을 안정화하고 가속화하는 기술입니다.
핵심 개념:
- 각 배치의 데이터 평균을 0, 분산을 1로 만들어 입력 분포를 정규화
- 활성화 함수 입력값의 분포가 일정하게 유지되도록 조절
주요 효과:
- 학습 속도 향상
- 더 높은 학습률 사용 가능
- 그래디언트 소실/폭발 문제 완화
- 정규화 효과로 드롭아웃 필요성 감소
동작 방식:
1. 배치별로 평균과 분산 계산
2. 정규화: (x - 평균) / sqrt(분산 + ε)
3. 스케일링과 이동: γx_norm + β (학습 가능한 파라미터)
보통 합성곱/완전연결층 다음, 활성화 함수 전에 위치합니다.
